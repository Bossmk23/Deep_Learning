# Deep Learning  â€“ Code Implementations 

This repository contains practical deep learning code implementations. It is paired with a separate whitepaper that includes theoretical insights and mathematical derivations.

##  Overview

This notebook demonstrates core deep learning concepts using both **TensorFlow** and **PyTorch**, including:

- Feedforward Neural Networks
- Backpropagation and Convergence Analysis
- Gradient Flow & Stability
- Normalization Techniques
- CNN, RNN, and Transformer Architectures

##  Technologies Used

- Python
- TensorFlow & PyTorch
- Matplotlib & Seaborn (for visualizations)
- Jupyter Notebook

##  Topics Covered

###  Neural Network Basics
- Architecture design
- Forward and backward propagation
- Activation functions (ReLU, Sigmoid, Tanh)

###  Gradient Challenges
- Vanishing/Exploding gradients
- Gradient tracking and visualization

###  Normalization Techniques
- Batch Normalization
- Layer Normalization

###  Deep Learning Architectures
- CNN on MNIST dataset
- RNN for character-level prediction
- Transformer for IMDb sentiment classification

##  Visualizations

The notebook includes visualizations such as:

- Loss and accuracy curves
- Gradient flow heatmaps and norms

These help explain training behavior and stability under different conditions.

---

>  Note: This repository focuses solely on code implementations. For theoretical background, derivations, and analysis, please refer to the accompanying whitepaper.
